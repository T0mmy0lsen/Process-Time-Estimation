{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import string\n",
    "\n",
    "from dateutil import parser\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "\n",
    "import objs.Request\n",
    "\n",
    "from objs.Request import Requests\n",
    "from importlib import reload\n",
    "from deep_translator import MyMemoryTranslator\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 331 entries, 0 to 333\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   id            331 non-null    int64         \n",
      " 1   description   331 non-null    object        \n",
      " 2   subject       331 non-null    object        \n",
      " 3   solution      331 non-null    object        \n",
      " 4   receivedDate  331 non-null    datetime64[ns]\n",
      " 5   solutionDate  331 non-null    object        \n",
      " 6   deadline      331 non-null    datetime64[ns]\n",
      " 7   priority      331 non-null    object        \n",
      " 8   hasError      331 non-null    bool          \n",
      " 9   process       331 non-null    int64         \n",
      " 10  cleanup       331 non-null    object        \n",
      "dtypes: bool(1), datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 28.8+ KB\n"
     ]
    }
   ],
   "source": [
    "reload(objs.Request)\n",
    "\n",
    "rs = Requests().get_between_sql(\n",
    "        str(datetime.datetime(2015, 12, 16)),\n",
    "        str(datetime.datetime(2015, 12, 17))\n",
    "    )\n",
    "\n",
    "def get_has_error(x):\n",
    "    return (\n",
    "        isinstance(x['solutionDate'], str)\n",
    "    )\n",
    "\n",
    "def get_process(x):\n",
    "    return int(x.solutionDate.timestamp()) - int(x.receivedDate.timestamp())\n",
    "\n",
    "def get_cleanup(x):\n",
    "    text = BeautifulSoup(x.description, \"lxml\").text\n",
    "    text = text.lower()\n",
    "    text = re.sub('[\\n.]', ' ', text)\n",
    "    return text\n",
    "\n",
    "data = pd.DataFrame(rs, columns=Requests().fillables)\n",
    "\n",
    "data['hasError'] = data.apply(lambda x: get_has_error(x), axis=1)\n",
    "data = data[~data['hasError']]\n",
    "\n",
    "data['process'] = data.apply(lambda x: get_process(x), axis=1)\n",
    "data['cleanup'] = data.apply(lambda x: get_cleanup(x), axis=1)\n",
    "data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "sentences, labels = list(data.cleanup), list(data.process)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_max_length(sequences):\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  tidligere efecte sag: inc-00372363 christian henrik jochimsen  lån af eksamens pc den 7 og 12 januar \n",
      "Into a sequence of int: [531, 2794, 115, 2795, 1890, 189, 2796, 737, 22, 1891, 155, 35, 375, 10, 40, 293]\n",
      "Into a padded sequence: [ 531 2794  115 ...    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[2])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = get_max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[2])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "  2\n",
      "sdu 3\n",
      "dk 4\n",
      "til 5\n",
      "i 6\n",
      "med 7\n",
      "er 8\n",
      "jeg 9\n",
      "og 10\n",
      "5803\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dropout, MaxPool1D, Flatten, Dense, Bidirectional, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def define_model(filters = 100, kernel_size = 3, activation='relu', input_dim = None, output_dim=300, max_length = None ):\n",
    "\n",
    "    # Channel 1\n",
    "    input1 = Input(shape=(max_length,))\n",
    "    embeddding1 = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_length)(input1)\n",
    "    conv1 = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',\n",
    "                   kernel_constraint= MaxNorm( max_value=3, axis=[0,1]))(embeddding1)\n",
    "    pool1 = MaxPool1D(pool_size=2, strides=2)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    drop1 = Dropout(0.5)(flat1)\n",
    "    dense1 = Dense(10, activation='relu')(drop1)\n",
    "    drop1 = Dropout(0.5)(dense1)\n",
    "    out1 = Dense(1, activation='sigmoid')(drop1)\n",
    "\n",
    "    # Channel 2\n",
    "    input2 = Input(shape=(max_length,))\n",
    "    embeddding2 = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_length, mask_zero=True)(input2)\n",
    "    gru2 = Bidirectional(GRU(64))(embeddding2)\n",
    "    drop2 = Dropout(0.5)(gru2)\n",
    "    out2 = Dense(1, activation='sigmoid')(drop2)\n",
    "\n",
    "    # Merge\n",
    "    merged = concatenate([out1, out2])\n",
    "\n",
    "    # Interpretation\n",
    "    outputs = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
    "\n",
    "    # Compile\n",
    "    model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 300)     300000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 98, 100)      90100       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 49, 100)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 4900)         0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4900)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 100, 300)     300000      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           49010       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 128)          140544      ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 10)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            11          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            129         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2)            0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            3           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 879,797\n",
      "Trainable params: 879,797\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Override the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('accuracy') > 0.93:\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0,\n",
    "                                             patience=7, verbose=2,\n",
    "                                             mode='auto', restore_best_weights=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['relu']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns=columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, shuffle=True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(data.cleanup), list(data.process)\n",
    "\n",
    "for activation in activations:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # kfold.split() will return set indices for each split\n",
    "        acc_list = []\n",
    "        for train, test in kfold.split(sentences):\n",
    "\n",
    "            train_x, test_x = [], []\n",
    "            train_y, test_y = [], []\n",
    "\n",
    "            for i in train:\n",
    "                train_x.append(sentences[i])\n",
    "                train_y.append(labels[i])\n",
    "\n",
    "            for i in test:\n",
    "                test_x.append(sentences[i])\n",
    "                test_y.append(labels[i])\n",
    "\n",
    "            # Turn the labels into a numpy array\n",
    "            train_y = np.array(train_y)\n",
    "            test_y = np.array(test_y)\n",
    "\n",
    "            # encode data using\n",
    "            # Cleaning and Tokenization\n",
    "            tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "            # Turn the text into sequence\n",
    "            training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "            test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "            max_len = get_max_length(training_sequences)\n",
    "\n",
    "            # Pad the sequence to have the same size\n",
    "            Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "            Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "            word_index = tokenizer.word_index\n",
    "            vocab_size = len(word_index) + 1\n",
    "\n",
    "            # Define the input shape\n",
    "            model = define_model(filters, kernel_size, activation, input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "            # Train the model and initialize test accuracy with 0\n",
    "            acc = 0\n",
    "            while acc < 0.7:\n",
    "\n",
    "                print('Training ...')\n",
    "\n",
    "                # Train the model\n",
    "                model.fit(x=[Xtrain, Xtrain], y = train_y, batch_size=50, epochs=100, verbose=1,\n",
    "                          callbacks=[callbacks], validation_data=([Xtest, Xtest], test_y))\n",
    "\n",
    "                # evaluate the model\n",
    "                loss, acc = model.evaluate([Xtest, Xtest], test_y, verbose=0)\n",
    "                print('Test Accuracy: {}'.format(acc * 100))\n",
    "\n",
    "                if acc < 0.7:\n",
    "                    print('The model suffered from local minimum. Retrain the model!')\n",
    "                    model = define_model(filters, kernel_size, activation, input_dim=vocab_size, max_length=max_len)\n",
    "                else:\n",
    "                    print('Done!')\n",
    "\n",
    "            # evaluate the model\n",
    "            loss, acc = model.evaluate([Xtest, Xtest], test_y, verbose=0)\n",
    "            print('Test Accuracy: {}'.format(acc * 100))\n",
    "\n",
    "            acc_list.append(acc * 100)\n",
    "\n",
    "        mean_acc = np.array(acc_list).mean()\n",
    "        parameters = [activation, kernel_size]\n",
    "        entries = parameters + acc_list + [mean_acc]\n",
    "\n",
    "        temp = pd.DataFrame([entries], columns=columns)\n",
    "        record = record.append(temp, ignore_index=True)\n",
    "        print()\n",
    "        print(record)\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}